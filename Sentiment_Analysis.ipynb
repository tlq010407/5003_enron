{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nrclex import NRCLex\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../enron_emails_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# praeparing the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text=df['Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment scores'] = [analyzer.polarity_scores(message) for message in text]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split based on time\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "#choose a time\n",
    "split_year=2000\n",
    "split_month=6\n",
    "\n",
    "time1_data=df[(df[\"Year\"]<split_year)|((df['Year']==split_year) & (df[\"Month\"]<split_month))]\n",
    "time2_data=df[(df[\"Year\"]>split_year)|((df['Year']==split_year) & (df[\"Month\"]>=split_month))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the positive and negative scores\n",
    "def determine_sentiment(data):\n",
    "    sentiment = []\n",
    "    for i in data['sentiment scores'].values:\n",
    "        if i['compound'] > 0.05:\n",
    "            sentiment.append('Positive')\n",
    "        elif i['compound'] < 0.05:\n",
    "            sentiment.append('Negative')\n",
    "        else:\n",
    "            sentiment.append('Neutral')\n",
    "    data['Sentiment'] = sentiment\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time1_data=determine_sentiment(time1_data)\n",
    "time2_data=determine_sentiment(time2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_values_time1 = time1_data['Sentiment'].value_counts()\n",
    "sentiment_values_time1.columns = ['Sentiment', 'Count']\n",
    "sentiment_values_time2 = time2_data['Sentiment'].value_counts()\n",
    "sentiment_values_time2.columns = ['Sentiment', 'Count']\n",
    "#print\n",
    "print(\"Sentiment values for time1:\")\n",
    "print(sentiment_values_time1)\n",
    "\n",
    "print(\"\\nSentiment values for time2:\")\n",
    "print(sentiment_values_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a graph for the sentiment as a funnel shaped graph\n",
    "fig_time1 = px.funnel(sentiment_values_time1, title='Sentiment Funnel for Time1')\n",
    "fig_time1.show()\n",
    "\n",
    "fig_time2 = px.funnel(sentiment_values_time2, title='Sentiment Funnel for Time2')\n",
    "fig_time2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!pip install nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')# split the sentence\n",
    "nltk.download('wordnet')#textblob\n",
    "nltk.download('vader_lexicon')  # VADER\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger') #lexical notation    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nrclex import NRCLex\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time1_data['Content'] = time1_data['Content'].astype(str)\n",
    "time2_data['Content'] = time2_data['Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining another function for sentiment analysis with different emotions using NRCLex lexicon\n",
    "def sentiment(message):\n",
    "    text = NRCLex(message)\n",
    "    if text.top_emotions[0][1] == 0.0:\n",
    "        return 'No Emotion'\n",
    "    else:\n",
    "        return text.top_emotions[0][0]\n",
    "time1_data['Emotions'] =  time1_data['Content'].apply(sentiment)\n",
    "time2_data['Emotions'] =  time2_data['Content'].apply(sentiment)\n",
    "\n",
    "time1_data.sample(5)\n",
    "time2_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_time1 = time1_data['Emotions'].value_counts()\n",
    "emotions_time2 = time2_data['Emotions'].value_counts()\n",
    "\n",
    "emotion_chart_time1 = pd.DataFrame(emotions_time1)\n",
    "emotion_chart_time2 = pd.DataFrame(emotions_time2)\n",
    "\n",
    "emotion_chart_time1 = emotion_chart_time1.drop('No Emotion', axis = 0)\n",
    "emotion_chart_time2 = emotion_chart_time2.drop('No Emotion', axis = 0)\n",
    "\n",
    "emotion_chart_time1, emotion_chart_time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pie chart for showing the percentage of every emotion in the text\n",
    "labels_time1 = emotion_chart_time1.index.tolist()\n",
    "labels_time2 = emotion_chart_time2.index.tolist()\n",
    "\n",
    "fig, axs=plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "axs[0].pie(emotion_chart_time1['count'].values, labels = labels_time1 , autopct = '%1.1f%%',labeldistance= 1.1)\n",
    "axs[0].set_title('Emotions - Time1', fontsize=14)\n",
    "\n",
    "axs[1].pie(emotion_chart_time2['count'].values, labels = labels_time2 , autopct = '%1.1f%%',labeldistance= 1.1)\n",
    "axs[1].set_title('Emotions - Time2', fontsize=14)\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
