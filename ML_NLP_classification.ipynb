{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning and NLP libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../enron_emails_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newstopwords = ['Re', 'FW', 'Fwd', 'EOL', 'E', 'mail', 'PLEASE', 'Ahead']\n",
    "for i in newstopwords:\n",
    "    stopwords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ' '.join(df['Subject'].values)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "wc = wordcloud.WordCloud(width = 800, height = 600, max_words = 200, stopwords = stopwords).generate(subjects)\n",
    "ax.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ' '.join(df['Body'].sample(5000).values)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "wc = wordcloud.WordCloud(width = 800, height = 600, max_words = 300, stopwords = stopwords).generate(content)\n",
    "ax.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning the body text\n",
    "def cleaningbody(col):\n",
    "    msgcol = []\n",
    "    for msg in col.values:\n",
    "        msg = re.sub(r'[<>\\n+\\t+\\s+\\*]', ' ', msg)\n",
    "        msg = re.sub(r'[0-9]+[a-zA-Z]+\\d+[?!].DOC', ' ', msg)\n",
    "        msg = re.sub(r'[?\\s+\\-+\\s+?_=~]', ' ', msg)\n",
    "        msg = re.sub(r' +', ' ', msg)\n",
    "        msg = msg.lower().strip(' ')\n",
    "        msgcol.append(msg)\n",
    "    return msgcol\n",
    "df['Body'] = cleaningbody(df['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the lemmatization function\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and lemmatizing the text to prepare for classification and sentiment analysis\n",
    "text = []\n",
    "for msg in df['Body'].values:\n",
    "    msg_tokens = word_tokenize(msg)\n",
    "    msg_tokens = [token.lower() for token in msg_tokens if token.isalpha()]\n",
    "    msg_tokens = [word for word in msg_tokens if not word in stopwords]\n",
    "    msg_tokens = [lemmatizer.lemmatize(word) for word in msg_tokens]\n",
    "    \n",
    "    text.append(msg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [' '.join(message) for message in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the data using Tfidfvectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_features = 5000, stop_words = stopwords, norm = 'l1')\n",
    "data = vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "data_norm = normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of the data and decreasing alot of features\n",
    "svd = TruncatedSVD(n_components = 2, n_iter = 10, random_state = 42)\n",
    "datasvd = svd.fit_transform(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasvd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Elbow method to define the optimal number of clusters for kmeans clustering\n",
    "max_iter = 1000\n",
    "sumsquares = []\n",
    "number_clusters = range(1,11)\n",
    "for i in number_clusters:\n",
    "    kmeans = KMeans(n_clusters = i, max_iter = max_iter, n_init = 'auto')\n",
    "    kmeans.fit(datasvd)\n",
    "    sumsquares.append(kmeans.inertia_)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(number_clusters, sumsquares)\n",
    "plt.xlabel('Clusters', fontsize = 14)\n",
    "plt.ylabel('Sum of Squared Distances', fontsize = 14)\n",
    "plt.title('Elbow Method', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "clf = KMeans(n_clusters = n_clusters,init = 'random', max_iter = max_iter, tol = 0.0001, algorithm = 'lloyd', n_init = 'auto', random_state = 42)\n",
    "fittedkmeans = clf.fit_predict(datasvd)\n",
    "centroids = clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A diagram showing the clusters\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.scatter(datasvd[:,0], datasvd[:,1], c = fittedkmeans, s = 50, cmap = 'viridis', alpha = 0.5)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s = 150, c = 'black', alpha = 0.8)\n",
    "plt.annotate('Cluster 0', xy = (centroids[0][0], centroids[0][1]), xytext = (centroids[0][0] + 0.02, centroids[0][1] + 0.03), color = 'white', fontsize = 12)\n",
    "plt.annotate('Cluster 1', xy = (centroids[1][0], centroids[1][1]), xytext = (centroids[1][0] + 0.02, centroids[1][1] + 0.02), color = 'white', fontsize = 12)\n",
    "plt.annotate('Cluster 2', xy = (centroids[2][0], centroids[2][1]), xytext = (centroids[2][0] - 0.05, centroids[2][1] - 0.06), color = 'white', fontsize = 12)\n",
    "plt.annotate('Cluster 3', xy = (centroids[3][0], centroids[3][1]), xytext = (centroids[3][0] - 0.01, centroids[3][1] - 0.07), color = 'white', fontsize = 12)\n",
    "plt.annotate('Cluster 4', xy = (centroids[4][0], centroids[4][1]), xytext = (centroids[4][0] - 0.03, centroids[4][1] + 0.04), color = 'white', fontsize = 12)\n",
    "plt.annotate('Cluster 5', xy = (centroids[5][0], centroids[5][1]), xytext = (centroids[5][0] - 0.03, centroids[5][1] + 0.03), color = 'white', fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top words in every cluster using the inverse_transform method\n",
    "original_space_centroids = svd.inverse_transform(kmeans.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1] #(10,5000)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i}: \", end=\"\")\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(f\"{terms[ind]} \", end=\"\")\n",
    "        \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
